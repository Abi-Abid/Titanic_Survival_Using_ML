{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Titanic Survival Prediction**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Introduction\n\nIn this project, we aim to predict the survival of passengers aboard the Titanic using machine learning. This is part of Kaggle’s beginner-friendly competition, **\"Titanic: Machine Learning from Disaster.\"** Kaggle provides us with two datasets: a **training set** (with survival outcomes) and a **test set** (without the target column).\n\nOur goal is to train a model using the training data and generate predictions on the test set. However, since the test set does **not include the `Survived` column**, we cannot directly evaluate our model's performance on it.\n\nTo overcome this, we'll first split the original training data into **training and validation subsets**. This allows us to train the model on one part and evaluate it on the other using metrics like the **classification report** and **confusion matrix**. Once we are satisfied with the model's performance, we will use it to make final predictions on the test data for submission.\n\nThis approach helps us build a more reliable and well-validated model.","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\n- Import the data from the Kaggle repository  \n- Perform data wrangling and preprocessing  \n- Create a machine learning pipeline  \n- Tune hyperparameters for optimal model performance  \n- Train the model on the training data using **Random Forest**  \n- Evaluate the model on the validation set  \n- Train a second model using **Logistic Regression**  \n- Evaluate and compare both models on the validation set  \n- Select the best-performing model and apply it to the test data  \n- Prepare the predictions for submission","metadata":{}},{"cell_type":"markdown","source":"## Import the data from the kaggle repository","metadata":{}},{"cell_type":"code","source":"#first let's import all required libraries\n\nimport pandas as pd #data manipulation, handle data in tabular format (DataFrames)\nimport numpy as np #numerical operations\nimport matplotlib.pyplot as plt #basic plotting, data visualization\nimport seaborn as sns #advanced statistical plots\n%matplotlib inline\n\nimport warnings #suppress the warning for better output\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.model_selection import train_test_split  # split data into training and validation sets\nfrom sklearn.model_selection import GridSearchCV  # for hyperparameter tuning using grid search\nfrom sklearn.model_selection import StratifiedKFold  # for cross-validation while preserving class distribution\n\nfrom sklearn.compose import ColumnTransformer  # apply different preprocessing to numerical and categorical features\nfrom sklearn.pipeline import Pipeline  # chain preprocessing and modeling steps together\nfrom sklearn.preprocessing import StandardScaler  #standardize numerical features (mean=0, std=1)\nfrom sklearn.preprocessing import OneHotEncoder  # convert categorical variables into binary dummy variables\n\nfrom sklearn.ensemble import RandomForestClassifier  # powerful ensemble method using decision trees\nfrom sklearn.linear_model import LogisticRegression  # linear model for binary classification\n\nfrom sklearn.metrics import classification_report  # get precision, recall, f1-score, etc.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # compute and visualize confusion matrix\n\nfrom sklearn.impute import SimpleImputer #handle missing values\nfrom sklearn.compose import ColumnTransformer #applying transormation to specific columns (numerical vs categorical)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T05:05:37.161391Z","iopub.execute_input":"2025-04-06T05:05:37.161763Z","iopub.status.idle":"2025-04-06T05:05:37.194139Z","shell.execute_reply.started":"2025-04-06T05:05:37.161735Z","shell.execute_reply":"2025-04-06T05:05:37.192838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import train and test data sets\ntrain_data = '/kaggle/input/titanic/train.csv'\ntest_data = '/kaggle/input/titanic/test.csv'\n\ntrain_df = pd.read_csv(train_data)\ntest_df = pd.read_csv(test_data)\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T04:13:09.946177Z","iopub.execute_input":"2025-04-06T04:13:09.946463Z","iopub.status.idle":"2025-04-06T04:13:09.992629Z","shell.execute_reply.started":"2025-04-06T04:13:09.946441Z","shell.execute_reply":"2025-04-06T04:13:09.991513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T04:13:45.102456Z","iopub.execute_input":"2025-04-06T04:13:45.102804Z","iopub.status.idle":"2025-04-06T04:13:45.113715Z","shell.execute_reply.started":"2025-04-06T04:13:45.102779Z","shell.execute_reply":"2025-04-06T04:13:45.112923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T04:15:31.947521Z","iopub.execute_input":"2025-04-06T04:15:31.947881Z","iopub.status.idle":"2025-04-06T04:15:31.954904Z","shell.execute_reply.started":"2025-04-06T04:15:31.947853Z","shell.execute_reply":"2025-04-06T04:15:31.953642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Therefore, our train data set have 891 rows and 12 columns","metadata":{}},{"cell_type":"code","source":"train_df.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T04:17:15.841403Z","iopub.execute_input":"2025-04-06T04:17:15.841722Z","iopub.status.idle":"2025-04-06T04:17:15.849990Z","shell.execute_reply.started":"2025-04-06T04:17:15.841698Z","shell.execute_reply":"2025-04-06T04:17:15.849215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the reults we can say that 'cabin' column have lot of missing values so let's drop the  column, and the columns 'age' and 'embarked' also have some missing values, so let's replace the 'age' column missing vcalues with the mean, and the 'embarked' missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"#drop 'survived' (target) and 'cabin' (too many missing values)\nX = train_df.drop(['Survived', 'Cabin', 'Name'], axis = 1)\ny = train_df['Survived']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:13:50.634153Z","iopub.execute_input":"2025-04-06T07:13:50.634458Z","iopub.status.idle":"2025-04-06T07:13:50.640178Z","shell.execute_reply.started":"2025-04-06T07:13:50.634432Z","shell.execute_reply":"2025-04-06T07:13:50.639069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### How balanced are the classes in the dataset?\nClaqss balance refe to whether the target variable ('Survived', inour data set) has roughly equal representation of each category (like, '0' = Not Survived, '1' = Survived). If one class is much more frequent than the other then the dataset is imbalanced.","metadata":{}},{"cell_type":"code","source":"y.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:13:53.622058Z","iopub.execute_input":"2025-04-06T07:13:53.622431Z","iopub.status.idle":"2025-04-06T07:13:53.630234Z","shell.execute_reply.started":"2025-04-06T07:13:53.622395Z","shell.execute_reply":"2025-04-06T07:13:53.628371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y.value_counts(normalize=True)*100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:13:55.901234Z","iopub.execute_input":"2025-04-06T07:13:55.901614Z","iopub.status.idle":"2025-04-06T07:13:55.909286Z","shell.execute_reply.started":"2025-04-06T07:13:55.901583Z","shell.execute_reply":"2025-04-06T07:13:55.908253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So about 38% of the passengers in the data set survived. Beacuse of this slight imbalance, we should stratify the data when performing train/test split and for cross-validation.\n\nWhat is stratify in Machine Learning and why is it important?\n\n`stratify` is a parameter used in `train_test_split()` to ensure that the train and test sets maintain the same class distribution as the original dataset. This is especially useful when the target variable (e.g., 'Survived' in the Titanic dataset) is imbalanced\n\nWhy use `stratify`?\n- Prevent class imbalance issues in training and testing data.\n- Ensure both train and test sets represent the original dataset's class distribution.\n- Avoids situations where one set has more survival cases than the other leading to biased models.\n\n- Without `stratify` the slit may result in an uneven distribution of survivors and non-survivors.\n- With `stratify`, both train and test sets will have the same proportion of survivors as the original dataset.","metadata":{}},{"cell_type":"code","source":"#SPLIT TRAIN SET INTO TRAIN AND VALIDATION SUBSETS\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:00.797912Z","iopub.execute_input":"2025-04-06T07:14:00.798293Z","iopub.status.idle":"2025-04-06T07:14:00.807464Z","shell.execute_reply.started":"2025-04-06T07:14:00.798262Z","shell.execute_reply":"2025-04-06T07:14:00.806305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Perform data wrangling and preprocessing\nNow let's define prepocessing transformers for numerical and categorical features, this will automatically detect numerical and categorical columns and assign them to separate numeric and categorical features.","metadata":{}},{"cell_type":"code","source":"numerical_features = X_train.select_dtypes(include = ['number']).columns.tolist()\ncategorical_features = X_train.select_dtypes(include = ['object', 'category']).columns.tolist()\n\nprint(numerical_features)\nprint(categorical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:03.292839Z","iopub.execute_input":"2025-04-06T07:14:03.293177Z","iopub.status.idle":"2025-04-06T07:14:03.299401Z","shell.execute_reply.started":"2025-04-06T07:14:03.293149Z","shell.execute_reply":"2025-04-06T07:14:03.298473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's create two preprocessing pipelines for numerical and categorical features. Each Pipeline automates the data cleaning and transformation process before feeding the data into a machine learning model. this helps in handling missing values and data standardization","metadata":{}},{"cell_type":"code","source":"numerical_features_transformer = Pipeline(steps = [('imputer', SimpleImputer(strategy = 'median')), ('scaler', StandardScaler())])\n\nCategorical_features_transformer = Pipeline(steps = [('imputer', SimpleImputer(strategy = 'most_frequent')), ('onehot', OneHotEncoder(handle_unknown = 'ignore'))])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:06.724853Z","iopub.execute_input":"2025-04-06T07:14:06.725203Z","iopub.status.idle":"2025-04-06T07:14:06.730230Z","shell.execute_reply.started":"2025-04-06T07:14:06.725175Z","shell.execute_reply":"2025-04-06T07:14:06.728700Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combine the transformers into a single column transformer\n\nWe'll use the sklearn 'column transformer' estimator to seperately transform the features, which will then concatenate the output as a single feature space, ready for input to a machine learning estimator.\n\nNote:\n- Pipeline = Step by step transformation for one type of data (numerical or ctegorical)\n- ColumnTransformer = Applies multiple pipelines (numerical + categorical) and combines them.","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers = [\n        ('numeric', numerical_features_transformer, numerical_features),\n        ('categorical', Categorical_features_transformer, categorical_features)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:11.895725Z","iopub.execute_input":"2025-04-06T07:14:11.896075Z","iopub.status.idle":"2025-04-06T07:14:11.900710Z","shell.execute_reply.started":"2025-04-06T07:14:11.896046Z","shell.execute_reply":"2025-04-06T07:14:11.899718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a machine learning pipeline\nNow let's create the model pipeline by combining the preprocessing with a Random Forest Classifier.","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(steps = [\n    ('preprocessing', preprocessor),\n    ('classifier', RandomForestClassifier(random_state = 42))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:15.616358Z","iopub.execute_input":"2025-04-06T07:14:15.616762Z","iopub.status.idle":"2025-04-06T07:14:15.621311Z","shell.execute_reply.started":"2025-04-06T07:14:15.616738Z","shell.execute_reply":"2025-04-06T07:14:15.619799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define a Parameter Grid\n\nA parameter Grid is a structed way to define a set of hyperparameters for tuning a machine learning model. It is commonly used in gris search to find the best combination of hyperparameters that optimize model performance. \n\nHow it works?\n\n1. A dictionary-like structure specifies different hyperparameter values.\n2. Model is trained and evaluated for each combination\n3. The best-performing set is chosen based on a scoring metic (e.g., accuracy, RMSE)\n\nFor example, Hyperparameters in random forest are:\n1. Number of trees (n_estimators) - More trees can make the model better, but too many may slow it down.\n2. Maximum depth of trees(max_depth) - A deeper tree capturesmore details but may overfit.\n3. Minimun samples per split (min_samples_split) - Control how much data is needed to split a node in a tree.\n\nLet's use the grid in a cross-validation search to optimize the model.","metadata":{}},{"cell_type":"code","source":"parameter_grid = {\n    'classifier__n_estimators': [50, 100], #2 options\n    'classifier__max_depth': [None, 10, 20], #3 options\n    'classifier__min_samples_split': [2,5] #2 options\n}\n\n#in total no of hyper parameter combinations = 2*3*2 = 12 candidates\n#GridSearchCV will train the model 12 times with different sets of hyperparameters. ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:19.147850Z","iopub.execute_input":"2025-04-06T07:14:19.148216Z","iopub.status.idle":"2025-04-06T07:14:19.153088Z","shell.execute_reply.started":"2025-04-06T07:14:19.148186Z","shell.execute_reply":"2025-04-06T07:14:19.151753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Perform grid search cross-validation and fit the best model to the training data\n\nIt means:\n1. Try different hyperparameter combinations (Grid search)\n2. Evaluate each combination using cross-validation (Cross-Validation)\n3. Find the best-pergorming combination\n4. Train the final model with the best parameters on the full training data\n\nStep-by step Explanation:\n1. Grid Search (Trying different settings)\n\nImagine we're baking a take and testinf different oven temperatures and baking times to find the best combination. Grid Search does this for machine learning models by testing multiple hyperparameter combinations.\n\n2. Cross-Validation (Ensuring stability)\n\nInstead of training the model on a single split of data, cross-validation splits the training data into multiple paets (folds), trains the model on some folds, and test on the others. This ensures the model works well across different data splits.\n\n3. Find the best Parameters\n\nAfter testing all combinations, the model picks the best hyperparameters based on a performance metric (e.g., accuracy, F1-score, RMSE). \n\n4. Fit the best model on full training data.\n\nOnce the best hyperparameters are found, a final model is trained on the entire training dataset using those parameters. This is the fianl model used for predictions.\n\nYou've already split the data into training and validation using train_test_split. That's a simple hold-out validation, and it's completely valid for early testing and fast model iteration.\n\n- Why use StratifiedKFold or cross-validation? - StratifiedKFold is part of cross-validation, which gives you a more robust estimate of your model's performance.\n\nWhen we're using GridSearchCV, it internally uses cross-validation (like StratifiedKFold) to evaluate how different hyperparameter combinations perform.\n\n- Instead of relying on a single split (which might be lucky or unlucky), cross-validation:\n- Trains on different parts of the training data\n- Validates on the remaining parts\n- Averages the performance\n- This makes your model selection less prone to overfitting on one split.","metadata":{}},{"cell_type":"markdown","source":"## Evaluate the model on the validation set","metadata":{}},{"cell_type":"code","source":"#cross-validation method\ncv = StratifiedKFold(n_splits = 5, shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:29.075878Z","iopub.execute_input":"2025-04-06T07:14:29.076186Z","iopub.status.idle":"2025-04-06T07:14:29.080388Z","shell.execute_reply.started":"2025-04-06T07:14:29.076162Z","shell.execute_reply":"2025-04-06T07:14:29.079273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train the pipeline model\nmodel = GridSearchCV(estimator = pipeline, param_grid = parameter_grid, cv = cv, scoring = 'accuracy', verbose = 2)\nmodel.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:29.845939Z","iopub.execute_input":"2025-04-06T07:14:29.846275Z","iopub.status.idle":"2025-04-06T07:14:37.937013Z","shell.execute_reply.started":"2025-04-06T07:14:29.846246Z","shell.execute_reply":"2025-04-06T07:14:37.935801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print the best parameter and best cross validation score\nprint('\\nBest Parameters Found: ', model.best_params_)\nprint('Best Cross-Validation Score: {:.2f}'.format(model.best_score_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:41.871384Z","iopub.execute_input":"2025-04-06T07:14:41.871746Z","iopub.status.idle":"2025-04-06T07:14:41.878367Z","shell.execute_reply.started":"2025-04-06T07:14:41.871713Z","shell.execute_reply":"2025-04-06T07:14:41.877144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#display model's estimated score\ntest_score = model.score(X_val, y_val)\nprint('Test set score: {:.2f}'.format(test_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:43.214526Z","iopub.execute_input":"2025-04-06T07:14:43.214956Z","iopub.status.idle":"2025-04-06T07:14:43.238963Z","shell.execute_reply.started":"2025-04-06T07:14:43.214922Z","shell.execute_reply":"2025-04-06T07:14:43.237894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Let's get the Model predictions from the grid search estimator on the unseen data, and print a classification model.\ny_pred = model.predict(X_val)\nprint(classification_report(y_val, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:44.396713Z","iopub.execute_input":"2025-04-06T07:14:44.397042Z","iopub.status.idle":"2025-04-06T07:14:44.426026Z","shell.execute_reply.started":"2025-04-06T07:14:44.397017Z","shell.execute_reply":"2025-04-06T07:14:44.424531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plot confusion matrix\nRandom_forest_conf_matrix = confusion_matrix(y_val, y_pred)\n\nplt.figure()\nsns.heatmap(Random_forest_conf_matrix, annot = True, cmap = 'Blues', fmt = 'd')\n\nplt.title('Titanic Classification Confusion Matrix - Random Forest')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:14:55.963286Z","iopub.execute_input":"2025-04-06T07:14:55.963723Z","iopub.status.idle":"2025-04-06T07:14:56.163703Z","shell.execute_reply.started":"2025-04-06T07:14:55.963682Z","shell.execute_reply":"2025-04-06T07:14:56.162565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insights\n- Class 0 (Did not survive) is predicted really well — high recall (0.93) means you're catching most of the actual \"did not survive\" cases.\n- Class 1 (Survived) has good precision (0.84), but relatively low recall (0.59) — meaning the model misses a lot of people who actually survived.\n- Overall Accuracy = 80%, which is solid, but there's room to improve recall on class 1.\n\n- True Negatives (0 → 0): 102 → Great, model correctly predicted many \"did not survive\".\n- False Positives (0 → 1): 8 → Not bad; a few were wrongly predicted as \"survived\".\n- False Negatives (1 → 0): 28 → This is the issue! These are real survivors the model missed.\n- True Positives (1 → 1): 41 → Decent, but ideally should be higher.\n\n- Our model is better at identifying non-survivors than survivors.\n- Low recall for class 1 suggests it's playing it safe and not “confident” enough to say someone survived.\n","metadata":{}},{"cell_type":"markdown","source":"## Train a second model using Logistic Regression\n","metadata":{}},{"cell_type":"code","source":"#Replace RandomForestClassifier with Logistic Regression\npipeline.set_params(classifier = LogisticRegression(random_state = 42))\n\n#update the model estimator to use thje new pipeline\nmodel.estimator = pipeline\n\n#define a new grid with logistic regression parameters\nparameter_grid_LR = {\n    'classifier__solver': ['liblinear'],\n    'classifier__penalty': ['l1', 'l2'],\n    'classifier__class_weight': [None, 'balanced']\n}\n\nmodel.param_grid = parameter_grid_LR\n\n#fit the updated pipeline with logistic regressionm\nmodel.fit(X_train, y_train)\n\n#make predictions\ny_pred = model.predict(X_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:15:06.358860Z","iopub.execute_input":"2025-04-06T07:15:06.359237Z","iopub.status.idle":"2025-04-06T07:15:06.807230Z","shell.execute_reply.started":"2025-04-06T07:15:06.359207Z","shell.execute_reply":"2025-04-06T07:15:06.805980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Display the classification report for the new model and compare the reults to our previous model\nclassification_report_lr = classification_report(y_val, y_pred)\nprint(classification_report_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:15:08.760791Z","iopub.execute_input":"2025-04-06T07:15:08.761137Z","iopub.status.idle":"2025-04-06T07:15:08.775528Z","shell.execute_reply.started":"2025-04-06T07:15:08.761108Z","shell.execute_reply":"2025-04-06T07:15:08.774354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"All of the scores are slightly better for logistic regression compared to Random Forest classification, although the differences are insignificant.","metadata":{}},{"cell_type":"code","source":"#display the confusion matrix for the new model and compare the results to our previous model.\n\nconfusion_matrix_lr = confusion_matrix(y_val, y_pred)\n\nplt.figure()\nsns.heatmap(confusion_matrix_lr, annot=True, cmap = 'Blues', fmt = 'd')\n\nplt.title('Titanic Classification Confusion Matrics')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:15:22.637738Z","iopub.execute_input":"2025-04-06T07:15:22.638090Z","iopub.status.idle":"2025-04-06T07:15:22.841451Z","shell.execute_reply.started":"2025-04-06T07:15:22.638061Z","shell.execute_reply":"2025-04-06T07:15:22.840439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insights\n- TN (True Negatives): 99 passengers were correctly predicted not to survive.\n- FP (False Positives): 11 passengers were incorrectly predicted to survive, but didn’t.\n- FN (False Negatives): 23 passengers were incorrectly predicted to not survive, but did.\n- TP (True Positives): 46 passengers were correctly predicted to survive.\n- Class 0 (Not Survived): Very high recall (0.90), meaning you're catching most of the non-survivors correctly.\n- Class 1 (Survived): Precision is solid (0.81), but recall is lower (0.67) — you're missing some actual survivors (as shown by the 23 FN in the confusion matrix).\n- Accuracy: 81% overall — very solid for a baseline model.\n- Macro Avg (average across classes): Balanced, but F1 could improve slightly for Class 1.\n- Good overall accuracy (81%).\n- Strong performance in identifying passengers who didn’t survive.\n- Balanced precision for both classes.\n","metadata":{}},{"cell_type":"markdown","source":"Now let's try other models\n- XGBoost\n- SVM","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\npipeline.set_params(classifier = XGBClassifier (random_state = 42, use_label_encoder = False, eval_metric = 'logloss'))\nmodel.estimator = pipeline\nparameter_grid_XGB = {\n    'classifier__n_estimators': [100, 200],\n    'classifier__max_depth': [3, 5, 7],\n    'classifier__learning_rate': [0.01, 0.1, 0.2],\n    'classifier__subsample': [0.8, 1]\n}\n\nmodel.param_grid = parameter_grid_XGB\nmodel.fit(X_train, y_train)\ny_pred_XGB = model.predict(X_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:15:29.862114Z","iopub.execute_input":"2025-04-06T07:15:29.862481Z","iopub.status.idle":"2025-04-06T07:15:51.244800Z","shell.execute_reply.started":"2025-04-06T07:15:29.862450Z","shell.execute_reply":"2025-04-06T07:15:51.244106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Display the classification report for the new model and compare the reults to our previous model\nclassification_report_XGB = classification_report(y_val, y_pred_XGB)\nprint(classification_report_XGB)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:15:54.593248Z","iopub.execute_input":"2025-04-06T07:15:54.593592Z","iopub.status.idle":"2025-04-06T07:15:54.607846Z","shell.execute_reply.started":"2025-04-06T07:15:54.593564Z","shell.execute_reply":"2025-04-06T07:15:54.606440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#display the confusion matrix for the new model and compare the results to our previous model.\n\nconfusion_matrix_XGB = confusion_matrix(y_val, y_pred_XGB)\n\nplt.figure()\nsns.heatmap(confusion_matrix_XGB, annot=True, cmap = 'Blues', fmt = 'd')\n\nplt.title('Titanic Classification Confusion Matrics')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:15:59.400300Z","iopub.execute_input":"2025-04-06T07:15:59.400791Z","iopub.status.idle":"2025-04-06T07:15:59.755788Z","shell.execute_reply.started":"2025-04-06T07:15:59.400755Z","shell.execute_reply":"2025-04-06T07:15:59.754205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#FOR SVM\nfrom sklearn.svm import SVC\n\n# Replace classifier in the pipeline\npipeline.set_params(classifier=SVC(kernel='linear', probability=True, random_state=42))\n\n# Update model's estimator\nmodel.estimator = pipeline\n\n# Define parameter grid for SVM\nparameter_grid_SVM = {\n    'classifier__C': [0.1, 1, 10],\n    'classifier__kernel': ['linear', 'rbf'],\n    'classifier__class_weight': [None, 'balanced'],\n}\n\n# Update model's param_grid\nmodel.param_grid = parameter_grid_SVM\n\n# Fit model\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred_SVM = model.predict(X_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:16:12.282214Z","iopub.execute_input":"2025-04-06T07:16:12.282542Z","iopub.status.idle":"2025-04-06T07:16:17.305047Z","shell.execute_reply.started":"2025-04-06T07:16:12.282515Z","shell.execute_reply":"2025-04-06T07:16:17.303770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Display the classification report for the new model and compare the reults to our previous model\nclassification_report_SVM = classification_report(y_val, y_pred_SVM)\nprint(classification_report_SVM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:16:26.783822Z","iopub.execute_input":"2025-04-06T07:16:26.784128Z","iopub.status.idle":"2025-04-06T07:16:26.800264Z","shell.execute_reply.started":"2025-04-06T07:16:26.784109Z","shell.execute_reply":"2025-04-06T07:16:26.799311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#display the confusion matrix for the new model and compare the results to our previous model.\n\nconfusion_matrix_SVM = confusion_matrix(y_val, y_pred_SVM)\n\nplt.figure()\nsns.heatmap(confusion_matrix_SVM, annot=True, cmap = 'Blues', fmt = 'd')\n\nplt.title('Titanic Classification Confusion Matrics')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:16:32.937931Z","iopub.execute_input":"2025-04-06T07:16:32.938244Z","iopub.status.idle":"2025-04-06T07:16:33.129633Z","shell.execute_reply.started":"2025-04-06T07:16:32.938215Z","shell.execute_reply":"2025-04-06T07:16:33.128205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate and compare FOUR models on the validation set\n\n| Model |\tAccuracy |\tRecall (Class 1)\t| Precision (Class 1)\t| F1 (Class 1)|\n|---|---|---|---|---|\n|Random Forest\t|0.79\t|0.64\t|0.79\t|0.70|\n|Logistic Reg.\t|0.81\t|0.67\t|0.81\t|0.73|\n|XGBoost|\t0.76|\t0.65\t|0.70|\t0.68|\n|SVM\t|0.81\t|0.71\t|0.78\t|0.74|\n\n- SVM is a strong performer here, especially for predicting survivors (good recall and precision).\n- Logistic Regression still has the highest overall accuracy, but SVM is extremely close and more balanced.\n- Random Forest and XGBoost are good but lean more toward class 0 prediction strength.","metadata":{}},{"cell_type":"markdown","source":"## Select the best-performing model and apply it to the test data\nTherefore our, final model is SVM, let's select the model and predict it onn the actual test data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_df = test_df.drop(columns=[\"Name\", \"Cabin\"])\n\n#Predict using the trained model pipeline\ntest_predictions = model.predict(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:27:57.616076Z","iopub.execute_input":"2025-04-06T07:27:57.616461Z","iopub.status.idle":"2025-04-06T07:27:57.644161Z","shell.execute_reply.started":"2025-04-06T07:27:57.616428Z","shell.execute_reply":"2025-04-06T07:27:57.642220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare the predictions for submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"PassengerId\": test_df[\"PassengerId\"],\n    \"Survived\": test_predictions\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:28:38.543499Z","iopub.execute_input":"2025-04-06T07:28:38.543896Z","iopub.status.idle":"2025-04-06T07:28:38.555892Z","shell.execute_reply.started":"2025-04-06T07:28:38.543869Z","shell.execute_reply":"2025-04-06T07:28:38.554500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:31:59.426864Z","iopub.execute_input":"2025-04-06T07:31:59.427198Z","iopub.status.idle":"2025-04-06T07:31:59.442193Z","shell.execute_reply.started":"2025-04-06T07:31:59.427178Z","shell.execute_reply":"2025-04-06T07:31:59.440575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a clickable download link\nFileLink(\"submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:34:41.523918Z","iopub.execute_input":"2025-04-06T07:34:41.524337Z","iopub.status.idle":"2025-04-06T07:34:41.534913Z","shell.execute_reply.started":"2025-04-06T07:34:41.524303Z","shell.execute_reply":"2025-04-06T07:34:41.532238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}